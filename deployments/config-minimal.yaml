# 最小化配置 - 仅用于测试 LLMProxy
# 请修改后端 URL 为你实际的服务地址

server:
  listen: ":8000"

backends:
  # 请修改为你的后端地址
  # 示例：
  # - OpenAI API: https://api.openai.com
  # - 本地 vLLM: http://host.docker.internal:8000
  # - 其他服务: http://your-service:port
  - name: default
    url: "http://host.docker.internal:8000"
    weight: 5

# 用量上报
usage:
  enabled: true
  reporters:
    - name: webhook
      type: webhook
      enabled: true
      webhook:
        url: "http://webhook-receiver:3001/llm-usage"
        timeout: 1s
        retry: 2

health_check:
  enabled: true
  interval: 10s
  path: /health
