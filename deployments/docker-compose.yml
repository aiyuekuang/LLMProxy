version: '3.8'

services:
  # vLLM 后端服务（示例）
  vllm:
    image: vllm/vllm-openai:latest
    command:
      - --model
      - meta-llama/Llama-3-8b-Instruct
      - --return-detailed-tokens  # 关键：启用 usage 返回
      - --port
      - "8000"
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  # LLMProxy 服务
  llmproxy:
    build:
      context: ..
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - ./config.yaml:/etc/llmproxy/config.yaml
    depends_on:
      - vllm
    environment:
      - TZ=Asia/Shanghai
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  # Prometheus（可选，用于监控）
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    depends_on:
      - llmproxy

  # Grafana（可选，用于可视化）
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana
    depends_on:
      - prometheus

volumes:
  grafana-storage:
