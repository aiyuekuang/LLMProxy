# LLMProxy 核心定位澄清

## 关键问题：我们到底对接什么？

### 当前产品定位（从 README 分析）

**明确答案：我们对接的是 vLLM/TGI 这类自建推理服务，不是 OpenAI/Claude 这类云端 API**

## 一、证据分析

### 1. README 中的描述
```
支持 vLLM、TGI、自研服务等 OpenAI 兼容后端
```

### 2. 配置示例
```yaml
backends:
  - url: "http://vllm:8000"      # 本地部署的 vLLM
    weight: 5
  - url: "http://tgi:8081"       # 本地部署的 TGI
    weight: 3
```

### 3. 后端配置要求
```bash
# vLLM 启动命令
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3-8b-Instruct \
  --return-detailed-tokens \
  --port 8000
```

### 4. Docker Compose 示例
包含 vLLM 服务，说明是和推理服务一起部署的

### 5. 架构图
```
LLMProxy
    ↓
vLLM (8000)  |  TGI (8081)
```

## 二、核心定位

### 我们是什么？

**"自建 LLM 推理服务的高性能网关"**

### 目标用户

1. **使用 vLLM/TGI 部署开源模型的团队**
   - 部署了 Llama、Mistral、Qwen 等开源模型
   - 需要负载均衡、健康检查、用量统计
   - 对性能要求高（实时对话）

2. **自研推理服务的公司**
   - 自己开发了推理引擎
   - 提供 OpenAI 兼容 API
   - 需要一个轻量级网关

3. **私有化部署的企业**
   - 数据不能出内网
   - 自建模型服务
   - 需要简单的管理和监控

### 不是我们的用户

1. ❌ 需要对接 OpenAI/Claude/Gemini 云端 API 的团队
   - 他们应该用 LiteLLM/One-API
   
2. ❌ 需要多模型聚合的 SaaS 服务商
   - 他们应该用 Portkey/One-API

3. ❌ 需要复杂权限管理的企业平台
   - 他们应该用 One-API

## 三、典型使用场景

### 场景 1：AI 对话应用（自建模型）

```
用户 App
    ↓
LLMProxy (负载均衡 + 用量统计)
    ↓
vLLM 集群 (3 个实例)
    ↓
Llama-3-70B 模型
```

**需求：**
- 高性能流式传输（实时对话）
- 负载均衡（3 个 vLLM 实例）
- 用量统计（计费）
- 健康检查（自动摘除故障节点）

**为什么选 LLMProxy：**
- 零缓冲，延迟最低
- 极简部署，无依赖
- Webhook 对接计费系统

---

### 场景 2：企业内部 AI 助手（私有化部署）

```
企业员工
    ↓
LLMProxy (鉴权 + 限流)
    ↓
TGI 服务
    ↓
Qwen-72B 模型（内网部署）
```

**需求：**
- 数据不出内网
- 简单的 API Key 鉴权
- 防止滥用（限流）
- 用量统计（内部核算）

**为什么选 LLMProxy：**
- 单二进制部署，无外部依赖
- 配置文件管理，无需数据库
- 轻量级，适合内网环境

---

### 场景 3：模型服务商（提供推理 API）

```
客户
    ↓
LLMProxy (鉴权 + 限流 + 计量)
    ↓
vLLM 集群 (多个模型)
    ↓
Llama-3, Mistral, Qwen 等
```

**需求：**
- 多个模型的负载均衡
- API Key 管理
- 用量计量（计费）
- 高性能（服务大量客户）

**为什么选 LLMProxy：**
- 性能极致
- 支持多后端
- Webhook 对接计费系统

## 四、与竞品的差异

### LiteLLM/One-API 的定位

**对接对象：** 云端 API（OpenAI、Claude、Gemini 等）

**使用场景：**
```
用户 App
    ↓
LiteLLM/One-API
    ↓
OpenAI API  |  Claude API  |  Gemini API
```

**核心功能：**
- 多模型聚合（100+ 提供商）
- 协议转换（OpenAI ↔ Claude ↔ Gemini）
- API Key 管理（多租户）
- 成本追踪（云端 API 计费）

---

### LLMProxy 的定位

**对接对象：** 自建推理服务（vLLM、TGI、自研）

**使用场景：**
```
用户 App
    ↓
LLMProxy
    ↓
vLLM  |  TGI  |  自研推理服务
```

**核心功能：**
- 高性能代理（零缓冲）
- 负载均衡（多实例）
- 健康检查（自动摘除）
- 用量统计（Webhook）

## 五、功能需求重新评估

基于"对接自建推理服务"的定位，重新评估功能需求：

### ✅ 应该实现的功能

1. **智能路由** ⭐⭐⭐⭐⭐
   - 模型映射（llama-3 -> vllm-instance-1）
   - 故障转移（主实例挂了切备用）
   - 自动重试（网络抖动）
   - **理由：** 提高可用性，自建服务更需要

2. **基础鉴权** ⭐⭐⭐⭐
   - API Key 验证
   - IP 白名单
   - 简单额度控制
   - **理由：** 防止内部滥用，必要的安全

3. **基础限流** ⭐⭐⭐⭐
   - 全局 QPS 限制
   - Key 级 QPS 限制
   - **理由：** 保护推理服务，防止打爆

4. **更多负载均衡策略** ⭐⭐⭐
   - 最少连接数
   - 响应时间优先
   - **理由：** 自建服务性能不均，需要智能调度

### ❌ 不应该实现的功能

1. **多模型协议转换** ❌
   - Claude、Gemini 协议转换
   - **理由：** 我们不对接云端 API，不需要

2. **响应缓存** ❌
   - **理由：** 违背零缓冲原则，且自建服务通常不需要缓存

3. **复杂成本追踪** ❌
   - **理由：** 自建服务成本固定（GPU 成本），不需要复杂计费

4. **多租户系统** ❌
   - **理由：** 过于复杂，简单的 API Key 就够了

## 六、最终定位

### Slogan

**"The Gateway for Self-Hosted LLM Inference Services"**

**"自建 LLM 推理服务的高性能网关"**

### 核心价值

1. **为 vLLM/TGI 而生** - 专门优化自建推理服务的场景
2. **极致性能** - 零缓冲，不增加延迟
3. **极简部署** - 单二进制，无依赖
4. **生产就绪** - 负载均衡、健康检查、监控

### 目标用户

- 使用 vLLM/TGI 部署开源模型的团队
- 自研推理服务的公司
- 私有化部署的企业

### 不是我们的用户

- 需要对接云端 API 的团队（用 LiteLLM）
- 需要多模型聚合的 SaaS（用 One-API）

## 七、功能路线图（基于新定位）

### Phase 1（核心功能）
1. ✅ 零缓冲流式代理
2. ✅ 负载均衡（轮询）
3. ✅ 健康检查
4. ✅ 用量统计（Webhook）
5. ✅ 监控指标

### Phase 2（增强功能）
6. ➕ 智能路由（模型映射、故障转移、重试）
7. ➕ 基础鉴权（API Key、IP 白名单）
8. ➕ 基础限流（全局 + Key 级）

### Phase 3（高级功能）
9. ➕ 更多负载均衡策略（最少连接、延迟优先）
10. ➕ 更多推理引擎支持（Ollama、LocalAI）

### 不做的功能
- ❌ 云端 API 对接（OpenAI、Claude、Gemini）
- ❌ 协议转换
- ❌ 响应缓存
- ❌ 复杂成本追踪
- ❌ Web UI

## 八、总结

### 核心定位

**LLMProxy = vLLM/TGI 的高性能网关**

### 与 LiteLLM/One-API 的区别

| 维度 | LLMProxy | LiteLLM/One-API |
|------|----------|-----------------|
| **对接对象** | 自建推理服务 | 云端 API |
| **典型后端** | vLLM、TGI | OpenAI、Claude |
| **核心价值** | 极致性能 | 多模型聚合 |
| **部署位置** | 和推理服务一起 | 独立部署 |
| **网络环境** | 内网 | 公网 |

### 我们解决的问题

1. **性能问题** - 自建推理服务需要极低延迟
2. **可用性问题** - 多实例负载均衡、故障转移
3. **管理问题** - 简单的鉴权、限流、监控
4. **计费问题** - 用量统计，对接计费系统

### 我们不解决的问题

1. ❌ 多云端 API 聚合
2. ❌ 协议转换
3. ❌ 复杂的多租户管理
4. ❌ 企业级权限系统

---

**结论：**

LLMProxy 是为 **vLLM/TGI 等自建推理服务** 设计的高性能网关，不是为云端 API 设计的聚合平台。

我们应该专注于：
1. 极致性能（零缓冲）
2. 高可用性（负载均衡、故障转移）
3. 简单管理（鉴权、限流、监控）

不应该做：
1. 多模型协议转换
2. 云端 API 对接
3. 复杂的平台功能

---

**文档版本：** v1.0  
**创建时间：** 2026-01-14
