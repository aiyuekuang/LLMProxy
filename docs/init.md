LLMProxyï¼šé¢å‘å¤§æ¨¡å‹æœåŠ¡çš„é«˜æ€§èƒ½ç½‘å…³  
â€”â€” æ”¯æŒæµå¼/éæµå¼æ— ç¼ä»£ç† + å¼‚æ­¥ç”¨é‡è®¡é‡ï¼ˆHTTP Webhookï¼‰

ç‰ˆæœ¬ï¼š1.0  
ç›®æ ‡ï¼šä¸º LLM æ¨ç†æœåŠ¡æä¾›è½»é‡ã€é«˜æ€§èƒ½ã€åè®®æ„ŸçŸ¥çš„åå‘ä»£ç†ï¼Œé›¶æ€§èƒ½æŸå¤±åœ°æ”¯æŒåç»­è®¡è´¹æ‰€éœ€çš„ token ä½¿ç”¨é‡ä¸ŠæŠ¥ã€‚

ä¸€ã€æ ¸å¿ƒéœ€æ±‚
éœ€æ±‚   è¯´æ˜
âœ… LLM åè®®æ„ŸçŸ¥ä»£ç†   è‡ªåŠ¨è¯†åˆ« /v1/chat/completions è¯·æ±‚ä¸­çš„ stream=true/falseï¼Œåˆ†åˆ«é€ä¼  SSE æµæˆ–å®Œæ•´ JSON

âœ… é›¶ç¼“å†²æµå¼ä¼ è¾“   SSE å“åº”é€ token è½¬å‘ï¼Œä¸å¢åŠ é¦– token å»¶è¿Ÿï¼ˆTTFTï¼‰

âœ… å¤šåç«¯è´Ÿè½½å‡è¡¡   æ”¯æŒ vLLMã€TGIã€è‡ªç ”æœåŠ¡ç­‰ OpenAI å…¼å®¹åç«¯

âœ… å¼‚æ­¥ç”¨é‡è®¡é‡   åœ¨è¯·æ±‚ç»“æŸåï¼Œåå°å¼‚æ­¥ä¸ŠæŠ¥ prompt_tokens + completion_tokens

âœ… é›¶æ€§èƒ½ä¾µå…¥   ä¸»è¯·æ±‚è·¯å¾„ä¸è§£æå“åº”ä½“ã€ä¸è¿æ¥æ•°æ®åº“ã€ä¸è°ƒç”¨å¤–éƒ¨æœåŠ¡

âœ… æç®€ä¸šåŠ¡å¯¹æ¥   é€šè¿‡ HTTP Webhook å°†ç”¨é‡æ•°æ®æ¨é€ç»™ä¸šåŠ¡ç³»ç»Ÿï¼Œä¸è¦æ±‚å›ºå®šè¡¨ç»“æ„

äºŒã€æ•´ä½“æ¶æ„

+------------------+
|     Client       | â† curl / SDK
+--------+---------+
         |
         | POST /v1/chat/completions { "stream": true, ... }
         v
+--------+---------+
|    LLMProxy      | â† Go æœåŠ¡ï¼ˆå•äºŒè¿›åˆ¶ï¼‰

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

|  â”‚  Router     â”‚â†â”€â”€ ä»…è·¯ç”± LLM API è·¯å¾„
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”

|  â”‚ LoadBalancerâ”‚â†â”€â”€ è½®è¯¢/æƒé‡/æœ€å°‘è¿æ¥
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”

|  â”‚ ProxyEngine â”‚â†â”€â”€ æ ¸å¿ƒï¼šé€ä¼ è¯·æ±‚/å“åº”ï¼ˆæ— ç¼“å†²ï¼ï¼‰
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”

|  â”‚ UsageHook   â”‚â†â”€â”€ è¯·æ±‚ç»“æŸåï¼Œå¯åŠ¨åå° goroutine
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜

|         | (async)
|         v
|  [HTTP Webhook] â”€â”€â”€â”€â†’ https://your-billing.com/usage
+------------------+

         â–¼
+------------------+     +------------------+
vLLM (8000)              TGI (8081)

+ usage                  + usage

+------------------+     +------------------+

ğŸ”‘ å…³é”®è®¾è®¡ï¼š  
- ä¸»è·¯å¾„ï¼šåªåš TCP-level é€ä¼ ï¼Œå»¶è¿Ÿ â‰ˆ ç½‘ç»œ RTT  
- è®¡é‡è·¯å¾„ï¼šå®Œå…¨å¼‚æ­¥ï¼Œå¤±è´¥ä¸å½±å“ä¸»æµç¨‹

ä¸‰ã€æŠ€æœ¯å®ç°ç»†èŠ‚

1. ä¸»ä»£ç†æµç¨‹ï¼ˆé«˜æ€§èƒ½ä¿éšœï¼‰

func proxyHandler(w http.ResponseWriter, r *http.Request) {
    // 1. ä»…å¤„ç† LLM è·¯å¾„
    if !isLLMEndpoint(r.URL.Path) {
        http.NotFound(w, r)
        return
    }

    // 2. è¯»å–åŸå§‹è¯·æ±‚ä½“ï¼ˆç”¨äºåç»­ç”¨é‡æå–ï¼‰
    bodyBytes, _ := io.ReadAll(r.Body)
    isStream := extractStreamFlag(bodyBytes) // å¿«é€Ÿ JSON è§£æ

    // 3. é€‰æ‹©åç«¯ & æ„é€ æ–°è¯·æ±‚
    backend := lb.Select()
    proxyReq := newRequest(backend, r.Method, r.URL.Path, bytes.NewReader(bodyBytes))

    // 4. å‘é€è¯·æ±‚ï¼ˆä½¿ç”¨å…±äº« HTTP clientï¼‰
    resp, err := httpClient.Do(proxyReq)
    if err != nil {
        http.Error(w, "Backend error", http.StatusBadGateway)
        return
    }
    defer resp.Body.Close()

    // 5. ã€å…³é”®ã€‘ç›´æ¥é€ä¼ å“åº”ï¼ˆä¸è§£æå†…å®¹ï¼ï¼‰
    if isStream {
        w.Header().Set("Content-Type", "text/event-stream")
        w.Header().Set("Cache-Control", "no-cache")
        w.Header().Set("Connection", "keep-alive")
        w.WriteHeader(http.StatusOK)
        io.Copy(w, resp.Body) // â† é›¶ç¼“å†²ï¼Œå®¢æˆ·ç«¯ç«‹å³æ”¶åˆ° token
    } else {
        w.Header().Set("Content-Type", "application/json")
        w.WriteHeader(resp.StatusCode)
        io.Copy(w, resp.Body)
    }

    // 6. ã€å¼‚æ­¥ã€‘è§¦å‘ç”¨é‡ä¸ŠæŠ¥ï¼ˆæ­¤æ—¶å®¢æˆ·ç«¯å·²æ”¶å®Œæ•°æ®ï¼ï¼‰
    go func() {
        usage := collectUsage(bodyBytes, resp, isStream, backend.URL)
        if usage != nil {
            webhookSender.SendAsync(usage) // éé˜»å¡
        }
    }()
}

âœ… æ€§èƒ½ä¿è¯ï¼š  
- io.Copy æ˜¯å†…æ ¸çº§ spliceï¼ŒCPU å¼€é”€æä½  
- ç”¨é‡æ”¶é›†åœ¨ go func() ä¸­ï¼Œä¸å½±å“ä¸» goroutine

2. ç”¨é‡æ”¶é›†ç­–ç•¥ï¼ˆå®‰å…¨ä¼˜å…ˆï¼‰
åç«¯ç±»å‹   æ˜¯å¦è¿”å› usage   LLMProxy è¡Œä¸º
vLLM   âœ… æ˜¯ï¼ˆéœ€ --return-detailed-tokensï¼‰   æå– usage.prompt_tokens / completion_tokens

TGI   âœ… æ˜¯ï¼ˆé»˜è®¤åœ¨æœ€åä¸€ä¸ª chunkï¼‰   è§£æ [DONE] å‰çš„ usage å­—æ®µ

å…¶ä»–   âŒ å¦   è·³è¿‡è®¡é‡ï¼ˆä¸ä¼°ç®—ï¼Œé¿å…æ€§èƒ½é£é™©ï¼‰

âš ï¸ å¼ºåˆ¶è¦æ±‚ï¼šä¸šåŠ¡æ–¹å¿…é¡»ç¡®ä¿åç«¯å¼€å¯ usage è¿”å›ã€‚  
ï¼ˆvLLM å¯åŠ¨å‚æ•°ç¤ºä¾‹è§ä¸‹æ–‡ï¼‰

3. HTTP Webhook ä¸ŠæŠ¥

è¯·æ±‚æ ¼å¼ï¼ˆPOST JSONï¼‰
{
  "request_id": "req_abc123",
  "user_id": "user_alice",
  "api_key": "sk-prod-xxx",
  "model": "meta-llama/Llama-3-8b",
  "prompt_tokens": 15,
  "completion_tokens": 42,
  "total_tokens": 57,
  "is_stream": true,
  "endpoint": "/v1/chat/completions",
  "timestamp": "2026-01-14T10:30:00Z",
  "backend_url": "http://vllm:8000"
}

é…ç½®ï¼ˆconfig.yamlï¼‰
usage_hook:
  enabled: true
  url: "https://billing.yourcompany.com/llm-usage"
  timeout: "1s"          # è¶…æ—¶çŸ­ï¼Œé¿å… goroutine é˜»å¡
  retry: 2               # å¤±è´¥é‡è¯•æ¬¡æ•°

ä¸šåŠ¡æ–¹åªéœ€ï¼š
- å®ç°ä¸€ä¸ªæ¥æ”¶æ¥å£ï¼ˆä»»ä½•è¯­è¨€ï¼‰
- æŒ‰éœ€å†™å…¥è‡ªå·±çš„æ•°æ®åº“è¡¨ï¼ˆå­—æ®µè‡ªç”±ï¼‰

Python Flask ç¤ºä¾‹
@app.route('/llm-usage', methods=['POST'])
def record_usage():
    data = request.json
    # ä½ çš„é€»è¾‘ï¼šINSERT INTO your_table (...)
    db.execute(
        "INSERT INTO billing_events (customer, input_tk, output_tk, model) VALUES (?, ?, ?, ?)",
        data['user_id'], data['prompt_tokens'], data['completion_tokens'], data['model']
    )
    return {"status": "ok"}

4. åç«¯å¯ç”¨ Usage çš„é…ç½®

vLLMï¼ˆå¿…éœ€ï¼‰
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3-8b \
  --return-detailed-tokens  # â† å…³é”®ï¼ä½¿å“åº”åŒ…å« usage

TGIï¼ˆé»˜è®¤æ”¯æŒï¼‰
- æ— éœ€é¢å¤–é…ç½®ï¼Œæœ€åä¸€ä¸ª SSE event åŒ…å« usageï¼š
    data: {"index":0,"finish_reason":"stop","usage":{"prompt_tokens":10,"completion_tokens":25}}

å››ã€éƒ¨ç½²ä¸è¿ç»´

é…ç½®æ–‡ä»¶ï¼ˆconfig.yamlï¼‰
listen: ":8080"

backends:
  - url: "http://vllm-1:8000"
    weight: 5
  - url: "http://tgi-1:8081"
    weight: 3

usage_hook:
  enabled: true
  url: "https://your-billing.com/llm-usage"
  timeout: "1s"

å¯é€‰ï¼šå¥åº·æ£€æŸ¥ã€é™é€Ÿç­‰
health_check:
  interval: "10s"
  path: "/health"

Docker éƒ¨ç½²
FROM golang:1.22 AS builder
WORKDIR /app
COPY . .
RUN go build -o llmproxy cmd/main.go

FROM alpine:latest
RUN apk --no-cache add ca-certificates
COPY --from=builder /app/llmproxy /usr/local/bin/
COPY config.yaml /etc/llmproxy/config.yaml
CMD ["llmproxy", "--config", "/etc/llmproxy/config.yaml"]

ç›‘æ§æŒ‡æ ‡ï¼ˆPrometheusï¼‰
è¯·æ±‚é‡
llmproxy_requests_total{stream="true", backend="..."}
å»¶è¿Ÿ
llmproxy_latency_ms_bucket{...}
Webhook æˆåŠŸç‡
llmproxy_webhook_success_total
llmproxy_webhook_failure_total

äº”ã€ä¸ºä»€ä¹ˆè¿™å¥—æ–¹æ¡ˆæœ€ä¼˜ï¼Ÿ
ç»´åº¦   ä¼ ç»Ÿæ–¹æ¡ˆï¼ˆNginx + Log Parserï¼‰   LLMProxy æ–¹æ¡ˆ
æµå¼æ”¯æŒ   éœ€æ‰‹åŠ¨å…³ proxy_bufferingï¼Œæ˜“å‡ºé”™   é»˜è®¤æ— ç¼“å†²ï¼Œå¼€ç®±å³ç”¨

ç”¨é‡è®¡é‡   éœ€ç¦»çº¿è§£ææ—¥å¿—ï¼Œå»¶è¿Ÿé«˜   è¯·æ±‚ç»“æŸç«‹å³ä¸ŠæŠ¥

æ€§èƒ½å½±å“   æ—¥å¿—å†™å…¥å¯èƒ½é˜»å¡   ä¸»è·¯å¾„é›¶å¼€é”€

ä¸šåŠ¡å¯¹æ¥   éœ€è§£æ Nginx æ—¥å¿—æ ¼å¼   ç›´æ¥æ¥æ”¶ç»“æ„åŒ– JSON

æ‰©å±•æ€§   éš¾ä»¥æ”¯æŒå¤šç§Ÿæˆ·/è®¡è´¹   åŸç”Ÿæºå¸¦ user_id/api_key

å…­ã€ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®

1. ç¡®ä¿åç«¯å¼€å¯ usageï¼ˆvLLM åŠ  --return-detailed-tokensï¼‰
2. éƒ¨ç½² LLMProxyï¼Œé…ç½® backend + webhook URL
3. ä¸šåŠ¡ç³»ç»Ÿå®ç° webhook æ¥æ”¶ç«¯
4. éªŒè¯ï¼š
      curl http://llmproxy:8080/v1/chat/completions -H "Content-Type: application/json" \
     -d '{"model":"Llama-3-8b","messages":[{"role":"user","content":"Hi"}],"stream":true}'
   
   â†’ æ£€æŸ¥ä¸šåŠ¡ç³»ç»Ÿæ˜¯å¦æ”¶åˆ°ç”¨é‡æ•°æ®

LLMProxy ä¸æ˜¯é€šç”¨ç½‘å…³ï¼Œè€Œæ˜¯ LLM æ—¶ä»£çš„ä¸“ç”¨æµé‡è°ƒåº¦å™¨ + è®¡é‡æ¢é’ˆã€‚  
å®ƒè®©é«˜æ€§èƒ½ä»£ç†ä¸å•†ä¸šè®¡è´¹ï¼ŒåŒæ—¶å˜å¾—ç®€å•ã€‚



å¤ªå¥½äº†ï¼ä»¥ä¸‹æ˜¯ **LLMProxy å®Œæ•´å¼€æºé¡¹ç›®æ¨¡æ¿**ï¼ŒåŒ…å«ï¼š

- âœ… æ ¸å¿ƒä»£ç†é€»è¾‘ï¼ˆæµå¼/éæµå¼ï¼‰
- âœ… å¼‚æ­¥ HTTP Webhook ç”¨é‡ä¸ŠæŠ¥
- âœ… Prometheus æŒ‡æ ‡ + å¥åº·æ£€æŸ¥
- âœ… Dockerfile + docker-compose.ymlï¼ˆå« vLLM åç«¯ç¤ºä¾‹ï¼‰
- âœ… Grafana ç›‘æ§é¢æ¿ JSON
- âœ… Helm Chartï¼ˆKubernetes éƒ¨ç½²ï¼‰

---

## ğŸ“¦ é¡¹ç›®ç»“æ„

```bash
llmproxy/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ main.go                 # å…¥å£
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ config/                 # é…ç½®è§£æ
â”‚   â”‚   â””â”€â”€ config.go
â”‚   â”œâ”€â”€ proxy/                  # æ ¸å¿ƒä»£ç†å¼•æ“
â”‚   â”‚   â”œâ”€â”€ handler.go          # è¯·æ±‚å¤„ç†
â”‚   â”‚   â””â”€â”€ usage_hook.go       # Webhook ä¸ŠæŠ¥
â”‚   â”œâ”€â”€ lb/                     # è´Ÿè½½å‡è¡¡å™¨
â”‚   â””â”€â”€ metrics/                # Prometheus æŒ‡æ ‡
â”œâ”€â”€ deployments/
â”‚   â”œâ”€â”€ docker-compose.yml      # æœ¬åœ°æµ‹è¯•ï¼ˆå« vLLMï¼‰
â”‚   â””â”€â”€ helm/                   # Kubernetes éƒ¨ç½²
â”‚       â””â”€â”€ llmproxy/
â”‚           â”œâ”€â”€ Chart.yaml
â”‚           â”œâ”€â”€ values.yaml
â”‚           â””â”€â”€ templates/
â”œâ”€â”€ grafana/
â”‚   â””â”€â”€ llmproxy-dashboard.json # Grafana é¢æ¿
â”œâ”€â”€ config.yaml.example         # é…ç½®ç¤ºä¾‹
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## ğŸš€ 1. æ ¸å¿ƒä»£ç ï¼ˆç²¾ç®€ç‰ˆï¼‰

### `cmd/main.go`
```go
package main

import (
	"flag"
	"log"
	"net/http"
	"os"

	"llmproxy/internal/config"
	"llmproxy/internal/metrics"
	"llmproxy/internal/proxy"
)

func main() {
	configPath := flag.String("config", "config.yaml", "path to config file")
	flag.Parse()

	cfg, err := config.Load(*configPath)
	if err != nil {
		log.Fatalf("Failed to load config: %v", err)
	}

	mux := http.NewServeMux()
	mux.HandleFunc("/metrics", metrics.Handler)
	mux.HandleFunc("/health", func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		w.Write([]byte("OK"))
	})
	mux.HandleFunc("/", proxy.NewHandler(cfg))

	log.Printf("Starting LLMProxy on %s", cfg.Listen)
	log.Fatal(http.ListenAndServe(cfg.Listen, mux))
}
```

---

### `internal/proxy/handler.go`
```go
package proxy

import (
	"bytes"
	"encoding/json"
	"io"
	"log"
	"net/http"
	"time"

	"llmproxy/internal/config"
	"llmproxy/internal/lb"
	"llmproxy/internal/metrics"
)

type RequestBody struct {
	Model  string `json:"model"`
	Stream bool   `json:"stream"`
}

func NewHandler(cfg *config.Config) http.HandlerFunc {
	client := &http.Client{}
	lb := lb.NewRoundRobin(cfg.Backends)

	return func(w http.ResponseWriter, r *http.Request) {
		start := time.Now()

		if r.URL.Path != "/v1/chat/completions" && r.URL.Path != "/v1/completions" {
			http.NotFound(w, r)
			return
		}

		if r.Method != "POST" {
			http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
			return
		}

		bodyBytes, err := io.ReadAll(r.Body)
		if err != nil {
			http.Error(w, "Bad request", http.StatusBadRequest)
			return
		}
		defer r.Body.Close()

		var reqBody RequestBody
		json.Unmarshal(bodyBytes, &reqBody) // å¿½ç•¥é”™è¯¯ï¼Œé»˜è®¤ stream=false

		backend := lb.Next()
		if backend == nil {
			http.Error(w, "No healthy backend", http.StatusServiceUnavailable)
			return
		}

		proxyReq, _ := http.NewRequest("POST", backend.URL+r.URL.Path, bytes.NewReader(bodyBytes))
		proxyReq.Header = r.Header.Clone()

		resp, err := client.Do(proxyReq)
		if err != nil {
			http.Error(w, "Backend error", http.StatusBadGateway)
			return
		}
		defer resp.Body.Close()

		// === å…³é”®ï¼šç›´æ¥é€ä¼ ï¼Œä¸è§£æå†…å®¹ ===
		if reqBody.Stream {
			w.Header().Set("Content-Type", "text/event-stream")
			w.Header().Set("Cache-Control", "no-cache")
			w.Header().Set("Connection", "keep-alive")
			w.WriteHeader(http.StatusOK)
			io.Copy(w, resp.Body)
		} else {
			w.Header().Set("Content-Type", "application/json")
			w.WriteHeader(resp.StatusCode)
			io.Copy(w, resp.Body)
		}

		// === å¼‚æ­¥ç”¨é‡ä¸ŠæŠ¥ ===
		go func() {
			usage := collectUsage(bodyBytes, resp, reqBody.Stream, backend.URL, reqBody.Model)
			if usage != nil {
				SendUsageWebhook(cfg.UsageHook, usage)
				metrics.RecordUsage(usage)
			}
		}()

		metrics.RecordRequest(r.URL.Path, reqBody.Stream, backend.URL, time.Since(start), resp.StatusCode)
	}
}
```

---

### `internal/proxy/usage_hook.go`
```go
package proxy

import (
	"bytes"
	"context"
	"encoding/json"
	"log"
	"net/http"
	"time"

	"llmproxy/internal/config"
)

type UsageRecord struct {
	RequestID        string    `json:"request_id"`
	UserID           string    `json:"user_id,omitempty"`
	APIKey           string    `json:"api_key,omitempty"`
	Model            string    `json:"model"`
	PromptTokens     int       `json:"prompt_tokens"`
	CompletionTokens int       `json:"completion_tokens"`
	TotalTokens      int       `json:"total_tokens"`
	IsStream         bool      `json:"is_stream"`
	Endpoint         string    `json:"endpoint"`
	Timestamp        time.Time `json:"timestamp"`
	BackendURL       string    `json:"backend_url"`
}

func collectUsage(reqBody []byte, resp *http.Response, isStream bool, backendURL, model string) *UsageRecord {
	// ç®€åŒ–ï¼šä»…å½“åç«¯è¿”å›å®Œæ•´ usage æ—¶æ‰è®°å½•ï¼ˆéæµå¼ or æµå¼æœ€åä¸€ä¸ª chunkï¼‰
	// å®é™…é¡¹ç›®ä¸­éœ€è§£æ SSE æµæ‰¾ [DONE] å‰çš„ usage
	if !isStream {
		var fullResp struct {
			Usage struct {
				PromptTokens     int `json:"prompt_tokens"`
				CompletionTokens int `json:"completion_tokens"`
				TotalTokens      int `json:"total_tokens"`
			} `json:"usage"`
		}
		body, _ := io.ReadAll(resp.Body)
		if json.Unmarshal(body, &fullResp) == nil && fullResp.Usage.PromptTokens > 0 {
			return &UsageRecord{
				Model:            model,
				PromptTokens:     fullResp.Usage.PromptTokens,
				CompletionTokens: fullResp.Usage.CompletionTokens,
				TotalTokens:      fullResp.Usage.TotalTokens,
				IsStream:         false,
				Endpoint:         "/v1/chat/completions",
				Timestamp:        time.Now(),
				BackendURL:       backendURL,
			}
		}
	}
	// æµå¼åœºæ™¯ï¼šæ­¤å¤„åº”è§£æ resp.Body ç¼“å­˜ï¼ˆç®€åŒ–èµ·è§æš‚ç•¥ï¼‰
	return nil
}

func SendUsageWebhook(hook *config.UsageHook, usage *UsageRecord) {
	if hook == nil || !hook.Enabled {
		return
	}

	data, _ := json.Marshal(usage)
	ctx, cancel := context.WithTimeout(context.Background(), hook.Timeout)
	defer cancel()

	req, _ := http.NewRequestWithContext(ctx, "POST", hook.URL, bytes.NewReader(data))
	req.Header.Set("Content-Type", "application/json")

	client := &http.Client{Timeout: hook.Timeout}
	resp, err := client.Do(req)
	if err != nil {
		log.Printf("Webhook failed: %v", err)
		return
	}
	resp.Body.Close()

	if resp.StatusCode < 200 || resp.StatusCode >= 300 {
		log.Printf("Webhook returned %d", resp.StatusCode)
	}
}
```

---

## ğŸ³ 2. Docker ä¸æœ¬åœ°æµ‹è¯•

### `Dockerfile`
```dockerfile
FROM golang:1.22 AS builder
WORKDIR /app
COPY . .
RUN CGO_ENABLED=0 go build -o llmproxy ./cmd

FROM alpine:latest
RUN apk --no-cache add ca-certificates
COPY --from=builder /app/llmproxy /usr/local/bin/
COPY config.yaml.example /etc/llmproxy/config.yaml
EXPOSE 8080
CMD ["llmproxy", "--config", "/etc/llmproxy/config.yaml"]
```

### `deployments/docker-compose.yml`
```yaml
version: '3'
services:
  vllm:
    image: vllm/vllm-openai:latest
    command: [
      "--model", "meta-llama/Llama-3-8b-Instruct",
      "--return-detailed-tokens",
      "--port", "8000"
    ]
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  llmproxy:
    build: ..
    ports:
      - "8080:8080"
    volumes:
      - ../config.yaml.example:/etc/llmproxy/config.yaml
    depends_on:
      - vllm
```

> ğŸ’¡ è¿è¡Œï¼š`docker compose -f deployments/docker-compose.yml up`

---

## ğŸ“Š 3. Grafana ç›‘æ§é¢æ¿ï¼ˆ`grafana/llmproxy-dashboard.json` ç‰‡æ®µï¼‰

```json
{
  "title": "LLMProxy Overview",
  "panels": [
    {
      "title": "Requests Rate",
      "targets": [{"expr": "rate(llmproxy_requests_total[1m])"}]
    },
    {
      "title": "P99 Latency (ms)",
      "targets": [{"expr": "histogram_quantile(0.99, rate(llmproxy_latency_ms_bucket[1m]))"}]
    },
    {
      "title": "Webhook Success Rate",
      "targets": [
        {"expr": "rate(llmproxy_webhook_success_total[1m])"},
        {"expr": "rate(llmproxy_webhook_failure_total[1m])"}
      ]
    }
  ]
}
```

> âœ… å®Œæ•´ JSON å¯æä¾›ï¼ˆæ”¯æŒå¯¼å…¥ Grafanaï¼‰

---

## â˜¸ï¸ 4. Helm Chartï¼ˆKubernetesï¼‰

### `deployments/helm/llmproxy/values.yaml`
```yaml
replicaCount: 2

image:
  repository: your-registry/llmproxy
  tag: v1.0

service:
  port: 8080

config:
  backends:
    - url: "http://vllm-service:8000"
      weight: 5
  usage_hook:
    enabled: true
    url: "https://billing.yourcompany.com/llm-usage"
    timeout: 1s
```

> âœ… æ”¯æŒ ConfigMap æ³¨å…¥é…ç½®ã€Prometheus ServiceMonitor

---

## ğŸ“„ 5. é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼ˆ`config.yaml.example`ï¼‰

```yaml
listen: ":8080"

backends:
  - url: "http://vllm:8000"
    weight: 5
  - url: "http://tgi:8081"
    weight: 3

usage_hook:
  enabled: true
  url: "https://your-billing.com/llm-usage"
  timeout: 1s
  retry: 2

# å¯é€‰
health_check:
  interval: 10s
  path: /health
```

---

## âœ… ä¸‹ä¸€æ­¥

ä½ å¯ä»¥ï¼š
1. **å…‹éš†æ­¤ç»“æ„**ï¼Œå¡«å……å®Œæ•´é€»è¾‘ï¼ˆæˆ‘å¯æä¾›å®Œæ•´ ZIP æˆ– GitHub repo æ¨¡æ¿ï¼‰
2. **é›†æˆ tokenizer fallback**ï¼ˆå¦‚éœ€ï¼‰
3. **æ·»åŠ  API Key é‰´æƒ**ï¼ˆä» header æå– user_idï¼‰

---

æ˜¯å¦éœ€è¦æˆ‘å°†ä¸Šè¿°å†…å®¹æ‰“åŒ…ä¸ºï¼š
- **GitHub ä»“åº“æ¨¡æ¿**ï¼ˆå« LICENSEã€Makefileã€CIï¼‰
- **æˆ–ä¸€ä¸ªå¯ç›´æ¥ `go run` çš„å®Œæ•´é¡¹ç›® ZIP**ï¼Ÿ

è¯·å‘Šè¯‰æˆ‘ä½ çš„åå¥½ï¼Œæˆ‘å¯ä»¥ç«‹å³ç”Ÿæˆï¼