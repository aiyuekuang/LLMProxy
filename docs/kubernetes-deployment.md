# Kubernetes éƒ¨ç½²æŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜å¦‚ä½•åœ¨ Kubernetes é›†ç¾¤ä¸­éƒ¨ç½² LLMProxyã€‚

## ğŸ“¦ åŸºç¡€éƒ¨ç½²

### 1. åˆ›å»º ConfigMap

```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: llmproxy-config
  namespace: default
data:
  config.yaml: |
    listen: ":8080"
    
    backends:
      - url: "http://vllm-service:8000"
        weight: 5
      - url: "http://tgi-service:8081"
        weight: 3
    
    usage_hook:
      enabled: true
      url: "https://billing-service.default.svc.cluster.local/llm-usage"
      timeout: 1s
      retry: 2
    
    health_check:
      interval: 10s
      path: /health
```

### 2. åˆ›å»º Deployment

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llmproxy
  namespace: default
  labels:
    app: llmproxy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llmproxy
  template:
    metadata:
      labels:
        app: llmproxy
    spec:
      containers:
      - name: llmproxy
        image: ghcr.io/aiyuekuang/llmproxy:v1.0.0
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        - containerPort: 9090
          name: metrics
          protocol: TCP
        volumeMounts:
        - name: config
          mountPath: /home/llmproxy/config.yaml
          subPath: config.yaml
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 5
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
      volumes:
      - name: config
        configMap:
          name: llmproxy-config
```

### 3. åˆ›å»º Service

```yaml
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: llmproxy
  namespace: default
  labels:
    app: llmproxy
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: http
  - port: 9090
    targetPort: 9090
    protocol: TCP
    name: metrics
  selector:
    app: llmproxy
```

### 4. éƒ¨ç½²

```bash
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
```

## ğŸŒ æš´éœ²æœåŠ¡

### æ–¹å¼ä¸€ï¼šIngressï¼ˆæ¨èï¼‰

```yaml
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llmproxy
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  ingressClassName: nginx
  rules:
  - host: llm.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llmproxy
            port:
              number: 8080
  tls:
  - hosts:
    - llm.example.com
    secretName: llm-tls
```

### æ–¹å¼äºŒï¼šLoadBalancer

```yaml
apiVersion: v1
kind: Service
metadata:
  name: llmproxy-lb
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: llmproxy
```

## ğŸ“Š ç›‘æ§é›†æˆ

### ServiceMonitorï¼ˆPrometheus Operatorï¼‰

```yaml
# k8s/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llmproxy
  namespace: default
  labels:
    app: llmproxy
spec:
  selector:
    matchLabels:
      app: llmproxy
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

## ğŸ”„ æ°´å¹³æ‰©å±•

### HorizontalPodAutoscaler

```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llmproxy
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llmproxy
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## ğŸ” å®‰å…¨åŠ å›º

### NetworkPolicy

```yaml
# k8s/networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: llmproxy
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: llmproxy
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: vllm
    ports:
    - protocol: TCP
      port: 8000
  - to:
    - podSelector:
        matchLabels:
          app: billing
    ports:
    - protocol: TCP
      port: 80
```

## ğŸ“¦ Helm Chartï¼ˆé«˜çº§ï¼‰

### åˆ›å»º Chart ç»“æ„

```bash
helm create llmproxy-chart
cd llmproxy-chart
```

### values.yaml

```yaml
replicaCount: 3

image:
  repository: ghcr.io/aiyuekuang/llmproxy
  pullPolicy: IfNotPresent
  tag: "v1.0.0"

service:
  type: ClusterIP
  port: 8080
  metricsPort: 9090

ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: llm.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: llm-tls
      hosts:
        - llm.example.com

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 128Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70

config:
  backends:
    - url: "http://vllm:8000"
      weight: 5
  usageHook:
    enabled: true
    url: "https://billing/usage"
    timeout: 1s
    retry: 2
```

### å®‰è£…

```bash
helm install llmproxy ./llmproxy-chart \
  --namespace llm \
  --create-namespace \
  --values custom-values.yaml
```

## ğŸš€ ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### 1. èµ„æºé…ç½®

```yaml
resources:
  requests:
    cpu: 500m      # ä¿è¯åŸºç¡€æ€§èƒ½
    memory: 256Mi
  limits:
    cpu: 2000m     # é˜²æ­¢å• Pod å ç”¨è¿‡å¤šèµ„æº
    memory: 1Gi
```

### 2. å¤šå‰¯æœ¬éƒ¨ç½²

```yaml
replicas: 3  # æœ€å°‘ 3 ä¸ªå‰¯æœ¬ï¼Œä¿è¯é«˜å¯ç”¨
```

### 3. Pod åäº²å’Œæ€§

```yaml
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - llmproxy
        topologyKey: kubernetes.io/hostname
```

### 4. ä¼˜é›…å…³é—­

```yaml
lifecycle:
  preStop:
    exec:
      command: ["/bin/sh", "-c", "sleep 15"]
terminationGracePeriodSeconds: 30
```

## ğŸ“ æ•…éšœæ’æŸ¥

### æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹æ‰€æœ‰ Pod æ—¥å¿—
kubectl logs -l app=llmproxy -n default --tail=100

# å®æ—¶è·Ÿè¸ª
kubectl logs -f deployment/llmproxy -n default
```

### æ£€æŸ¥å¥åº·çŠ¶æ€

```bash
# æŸ¥çœ‹ Pod çŠ¶æ€
kubectl get pods -l app=llmproxy -n default

# æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
kubectl describe pod <pod-name> -n default

# è¿›å…¥å®¹å™¨è°ƒè¯•
kubectl exec -it <pod-name> -n default -- /bin/sh
```

### æµ‹è¯•è¿æ¥

```bash
# ç«¯å£è½¬å‘åˆ°æœ¬åœ°
kubectl port-forward svc/llmproxy 8080:8080 -n default

# æµ‹è¯•è¯·æ±‚
curl http://localhost:8080/health
```

## ğŸ”„ æ»šåŠ¨æ›´æ–°

```bash
# æ›´æ–°é•œåƒç‰ˆæœ¬
kubectl set image deployment/llmproxy \
  llmproxy=ghcr.io/aiyuekuang/llmproxy:v1.1.0 \
  -n default

# æŸ¥çœ‹æ›´æ–°çŠ¶æ€
kubectl rollout status deployment/llmproxy -n default

# å›æ»šï¼ˆå¦‚æœ‰é—®é¢˜ï¼‰
kubectl rollout undo deployment/llmproxy -n default
```

## ğŸ“š å‚è€ƒèµ„æº

- [Kubernetes å®˜æ–¹æ–‡æ¡£](https://kubernetes.io/docs/)
- [Helm æ–‡æ¡£](https://helm.sh/docs/)
- [Prometheus Operator](https://prometheus-operator.dev/)
